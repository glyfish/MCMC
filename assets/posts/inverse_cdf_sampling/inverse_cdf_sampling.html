
<p>
Inverse <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">CDF</a> sampling is a method for obtaining samples from both discrete and continuous probability distributions that requires the CDF to be invertable. The method assumes values of the CDF are Uniform random variables on [0, 1]. CDF values are generated and used as input into the inverted CDF to obain samples with the distribution defined by the CDF.
<p>
  <h2>Sampling Discrete Distributions</h2>
<p>
A discrete probability distribution consisting of a finite set of [katex] {N}[/katex] probability values is defined by, [katex] {\{p_1, p_2,\ldots,p_N\}}[/katex] with [katex] {p_i \geq 0, \forall i}[/katex] and [katex] {\sum_{i=1}^N{p_i} = 1.}[/katex] The CDF specifies the probability that [katex] {i \leq n}[/katex] and is given by, <p align=center>[katex display="true"]   P(i \leq n)=P(n)=\sum_{i=1}^n{p_i}, \ \ \ \ \ (1)[/katex]</p>
 where [katex] {P(N)=1.}[/katex]
<p>
For a given generated CDF value, [katex] {u}[/katex], Equation (1) can always be inverted by evaluating it for each [katex] {n}[/katex] and searching for the value of [katex] {n}[/katex] that satisfies, [katex] {P(n) \geq u.}[/katex] It can be seen that the generated samples will have distribution [katex] {\{p_n\}}[/katex] since the intervals [katex] {P(n)-P(n-1) = p_n}[/katex] are Uniformly sampled.
<p>
Consider the distribution,
<p>
<p align=center>[katex display="true"]   \left\{\frac{1}{12}, \frac{1}{12}, \frac{1}{6}, \frac{1}{6}, \frac{1}{12}, \frac{5}{12} \right\}  \ \ \ \ \ (2)[/katex]</p>

<p>
It is shown in the following plot with its CDF.
<p>
 <p align=center><img width = 600 src="https://gly.fish/wp-content/uploads/posts/inverse-cdf-sampling/discrete_cdf.png"></p>
<p>
A sampler using the Inverse CDF method can be implemented in Python in a few lines of code,
<p>
  <pre class="EnlighterJSRAW" data-enlighter-language="python">
<p>
import numpy
<p>
n = 10000 df = numpy.array([1/12, 1/12, 1/6, 1/6, 1/12, 5/12]) cdf = numpy.cumsum(df)
<p>
cdf_star = numpy.random.rand(n) samples = [numpy.flatnonzero(cdf >= cdf_star[i])[0] for i in range(n)]
<p>
</pre>
<p>

<p>
The figure below favorably compares generated samples and distribution (2),  <p align=center><img width = 600 src="https://gly.fish/wp-content/uploads/posts/inverse-cdf-sampling/discrete_sampled_distribution.png"></p>
<p>
  It is also possible to directly sample [katex] {\{p_n\}}[/katex] using the <code>multinomial</code> sampler from <code>numpy</code>,
<p>
<pre class="EnlighterJSRAW" data-enlighter-language="python"> import numpy
<p>
n = 10000 df = numpy.array([1/12, 1/12, 1/6, 1/6, 1/12, 5/12]) samples = numpy.random.multinomial(n, df, size=1)/n </pre>
<p>

<p>
  <h2>Sampling Continuous Distributions</h2>
<p>
A continuous probability distribution is defined by the <a href="https://en.wikipedia.org/wiki/Probability_density_function">PDF</a>, [katex] {f_X(x)}[/katex], where [katex] {f_X(x) \geq 0, \forall x}[/katex] and [katex] {\int f_X(x) dx = 1.}[/katex] The CDF is a monotonically increasing function that specifies the probability that [katex] {X \leq x}[/katex], namely, <p align=center>[katex display="true"]   P(X \leq x) = F_X(x) = \int^{x} f_X(w) dw. \ \ \ \ \ (3)[/katex]</p>

<p>
  <h3>Proof</h3>
<p>
To prove that Inverse CDF sampling works for continuos distributions it must be shown that,
<p>
<p align=center>[katex display="true"]   P[F_X^{-1}(U) \leq x] = F_X(x), \ \ \ \ \ (4)[/katex]</p>

<p>
where [katex] {F_X^{-1}(x)}[/katex] is the inverse of [katex] {F_X(x)}[/katex] and [katex] {U \sim \textbf{Uniform}(0, 1)}[/katex]. A more general result needed to complete this proof is obtained using a change of variables on a CDF. If [katex] {Y=G(X)}[/katex] is a monotonically increasing invertable function of [katex] {X}[/katex] then <p align=center>[katex display="true"]   P(X \leq x) = P(Y \leq y) = P[G(X) \leq G(x)]. \ \ \ \ \ (5)[/katex]</p>

<p>
To prove this note that [katex] {G(x)}[/katex] is monotonically increasing so the ordering of values is preserved,
<p>
<p align=center>[katex display="true"]  X \le x \implies G(X) \le G(x).[/katex]</p>

<p>
Consequently, the order of the integration limits is maintained by the transformation. Futher, since [katex] {G(x)}[/katex] is invertable, [katex] {x = G^{-1}(y)}[/katex] and [katex] {dx = \frac{dG^{-1}}{dy} dy}[/katex], so
<p>
<p align=center>[katex display="true"]  \begin{aligned} P(X \leq x) & = \int^{x} f_X(w) dw \\ & = \int^{y} f_X(G^{-1}(z)) \frac{dG^{-1}}{dz} dz \\ & = \int^{y} f_Y(z) dz \\ & = P(Y \leq y) \\ & = P[G(X) \leq G(x)], \end{aligned} [/katex]</p>

<p>
where,
<p>
<p align=center>[katex display="true"]  f_Y(y) = f_X(G^{-1}(y)) \frac{dG^{-1}}{dy} [/katex]</p>

<p>
The proof of Equation (4) follows from Equation (5), using [katex] {f_U(u) = 1}[/katex] since [katex] {U \sim \textbf{Uniform}(0, 1),}[/katex]
<p>
<p align=center>[katex display="true"]  \begin{aligned} P[F_X^{-1}(U) \leq x] & = P[F_X(F_X^{-1}(U)) \leq F_X(x)] \\ & = P[U \leq F_X(x)] \\ & = \int_{0}^{F_X(x)} f_U(w) dw \\ & = \int_{0}^{F_X(x)} dw \\ & = F_X(x). \end{aligned} [/katex]</p>

<p>
  <h3>Example</h3>
<p>
Consider the <a href="https://en.wikipedia.org/wiki/Weibull_distribution">Weibull Distribution</a>, with density
<p>
<p align=center>[katex display="true"]   f_X(x; k, \lambda) = \begin{cases} \frac{k}{\lambda}\left(\frac{x}{\lambda} \right)^{k-1} e^{\left(\frac{-x}{\lambda}\right)^k} & x \geq 0 \\ 0 & x lt 0, \end{cases} \ \ \ \ \ (6)[/katex]</p>

<p>
and CDF,
<p>
<p align=center>[katex display="true"]   F_X(x; k, \lambda) = \begin{cases} 1-e^{\left(\frac{-x}{\lambda}\right)^k} & x \geq 0 \\ 0 & x &lt; 0. \end{cases} \ \ \ \ \ (7)[/katex]</p>

<p>
Equation (7) can be inverted to yield,
<p>
<p align=center>[katex display="true"]   F_X^{-1}(u; k, \lambda) = \begin{cases} \lambda\ln\left(\frac{1}{1-u}\right)^{\frac{1}{k}} & 0 \leq u \geq 1 \\ 0 & u &lt; 0 \text{ or } u &gt; 1. \end{cases} \ \ \ \ \ (8)[/katex]</p>

<p>
